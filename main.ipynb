{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mô hình Autoformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    def __init__(self,size, path, flag, col_input = [2,3,4,5,6], col_output = [2,3,4,5]):\n",
    "        super().__init__()\n",
    "        self.seq_len = size[0]\n",
    "        self.lab_len = size[1]\n",
    "        self.pred_len = size[2]\n",
    "        self.col_input = col_input\n",
    "        self.col_output = col_output\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        assert flag in ['train', 'test', 'valid']\n",
    "        type_map = {'train': 0, 'valid': 1, 'test': 2}\n",
    "        self.set_type = type_map[flag]\n",
    "        self.path = path\n",
    "        self.__read_data__()\n",
    "    \n",
    "    def __read_data__(self):     \n",
    "        df_raw = pd.read_csv(self.path)\n",
    "        border1s = [0, int(len(df_raw)*0.6) - self.seq_len, int(len(df_raw)* 0.8) - self.seq_len]\n",
    "        border2s = [int(len(df_raw)*0.6), int(len(df_raw)*0.8), int(len(df_raw))]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "        self.data_x = torch.tensor(df_raw[df_raw.columns[self.col_input]].values[border1:border2], dtype = torch.float32)\n",
    "        self.data_y = torch.tensor(df_raw[df_raw.columns[self.col_output]].values[border1:border2], dtype = torch.float32)    \n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = index + self.seq_len\n",
    "        r_begin = s_end - self.lab_len\n",
    "        r_end = r_begin + self.lab_len + self.pred_len\n",
    "        seq_x = self.data_x[s_begin: s_end]\n",
    "        seq_y = self.data_y[r_begin: r_end]\n",
    "        return seq_x, seq_y, seq_x, seq_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_x) -  self.seq_len - self.pred_len + 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoCorrelation(nn.Module):\n",
    "    \"\"\"\n",
    "    AutoCorrelation Mechanism with the following two phases:\n",
    "    (1) period-based dependencies discovery\n",
    "    (2) time delay aggregation\n",
    "    This block can replace the self-attention family mechanism seamlessly.\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_flag=True, factor=1, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(AutoCorrelation, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def time_delay_agg_training(self, values, corr):\n",
    "        \"\"\"\n",
    "        SpeedUp version of Autocorrelation (a batch-normalization style design)\n",
    "        This is for the training phase.\n",
    "        \"\"\"\n",
    "        head = values.shape[1]\n",
    "        channel = values.shape[2]\n",
    "        length = values.shape[3]\n",
    "        # find top k\n",
    "        top_k = int(self.factor * math.log(length))\n",
    "        mean_value = torch.mean(torch.mean(corr, dim=1), dim=1)\n",
    "        index = torch.topk(torch.mean(mean_value, dim=0), top_k, dim=-1)[1]\n",
    "        weights = torch.stack([mean_value[:, index[i]] for i in range(top_k)], dim=-1)\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # aggregation\n",
    "        tmp_values = values\n",
    "        delays_agg = torch.zeros_like(values).float()\n",
    "        for i in range(top_k):\n",
    "            pattern = torch.roll(tmp_values, -int(index[i]), -1)\n",
    "            delays_agg = delays_agg + pattern * \\\n",
    "                         (tmp_corr[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length))\n",
    "        return delays_agg\n",
    "\n",
    "    def time_delay_agg_inference(self, values, corr):\n",
    "        \"\"\"\n",
    "        SpeedUp version of Autocorrelation (a batch-normalization style design)\n",
    "        This is for the inference phase.\n",
    "        \"\"\"\n",
    "        batch = values.shape[0]\n",
    "        head = values.shape[1]\n",
    "        channel = values.shape[2]\n",
    "        length = values.shape[3]\n",
    "        # index init\n",
    "        init_index = torch.arange(length).unsqueeze(0).unsqueeze(0).unsqueeze(0)\\\n",
    "            .repeat(batch, head, channel, 1).to(values.device)\n",
    "        # find top k\n",
    "        top_k = int(self.factor * math.log(length))\n",
    "        mean_value = torch.mean(torch.mean(corr, dim=1), dim=1)\n",
    "        weights, delay = torch.topk(mean_value, top_k, dim=-1)\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # aggregation\n",
    "        tmp_values = values.repeat(1, 1, 1, 2)\n",
    "        delays_agg = torch.zeros_like(values).float()\n",
    "        for i in range(top_k):\n",
    "            tmp_delay = init_index + delay[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length)\n",
    "            pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)\n",
    "            delays_agg = delays_agg + pattern * \\\n",
    "                         (tmp_corr[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length))\n",
    "        return delays_agg\n",
    "\n",
    "    def time_delay_agg_full(self, values, corr):\n",
    "        \"\"\"\n",
    "        Standard version of Autocorrelation\n",
    "        \"\"\"\n",
    "        batch = values.shape[0]\n",
    "        head = values.shape[1]\n",
    "        channel = values.shape[2]\n",
    "        length = values.shape[3]\n",
    "        # index init\n",
    "        init_index = torch.arange(length).unsqueeze(0).unsqueeze(0).unsqueeze(0)\\\n",
    "            .repeat(batch, head, channel, 1).to(values.device)\n",
    "        # find top k\n",
    "        top_k = int(self.factor * math.log(length))\n",
    "        weights, delay = torch.topk(corr, top_k, dim=-1)\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # aggregation\n",
    "        tmp_values = values.repeat(1, 1, 1, 2)\n",
    "        delays_agg = torch.zeros_like(values).float()\n",
    "        for i in range(top_k):\n",
    "            tmp_delay = init_index + delay[..., i].unsqueeze(-1)\n",
    "            pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)\n",
    "            delays_agg = delays_agg + pattern * (tmp_corr[..., i].unsqueeze(-1))\n",
    "        return delays_agg\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        if L > S:\n",
    "            zeros = torch.zeros_like(queries[:, :(L - S), :]).float()\n",
    "            values = torch.cat([values, zeros], dim=1)\n",
    "            keys = torch.cat([keys, zeros], dim=1)\n",
    "        else:\n",
    "            values = values[:, :L, :, :]\n",
    "            keys = keys[:, :L, :, :]\n",
    "\n",
    "        # period-based dependencies\n",
    "        q_fft = torch.fft.rfft(queries.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(keys.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, n=L, dim=-1)\n",
    "\n",
    "        # time delay agg\n",
    "        if self.training:\n",
    "            V = self.time_delay_agg_training(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
    "        else:\n",
    "            V = self.time_delay_agg_inference(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return (V.contiguous(), corr.permute(0, 3, 1, 2))\n",
    "        else:\n",
    "            return (V.contiguous(), None)\n",
    "\n",
    "\n",
    "class AutoCorrelationLayer(nn.Module):\n",
    "    def __init__(self, correlation, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None):\n",
    "        super(AutoCorrelationLayer, self).__init__()\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "        self.inner_correlation = correlation\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads, device = device)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads, device = device)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads, device = device)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model, device = device)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_correlation(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask\n",
    "        )\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out), attn\n",
    "\n",
    "\n",
    "class my_Layernorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Special designed layernorm for the seasonal part\n",
    "    \"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super(my_Layernorm, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(channels, device= 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.layernorm(x)\n",
    "        bias = torch.mean(x_hat, dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)\n",
    "        return x_hat - bias\n",
    "    \n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x    \n",
    "    \n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean    \n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder layer with the progressive decomposition architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, attention, d_model, d_ff=None, moving_avg=25, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False,  device= 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False,  device= 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.decomp1 = series_decomp(moving_avg)\n",
    "        self.decomp2 = series_decomp(moving_avg)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "        x, _ = self.decomp1(x)\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        res, _ = self.decomp2(x + y)\n",
    "        return res, attn    \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns    \n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer decoder layer with the progressive decomposition architecture\n",
    "    \"\"\"\n",
    "    def __init__(self, self_attention, cross_attention, d_model, c_out, d_ff=None,\n",
    "                 moving_avg=25, dropout=0.1, activation=\"relu\"):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False,  device= 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False,  device= 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.decomp1 = series_decomp(moving_avg)\n",
    "        self.decomp2 = series_decomp(moving_avg)\n",
    "        self.decomp3 = series_decomp(moving_avg)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.projection = nn.Conv1d(in_channels=d_model, out_channels=c_out, kernel_size=3, stride=1, padding=1,\n",
    "                                    padding_mode='circular', bias=False,  device= 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        x = x + self.dropout(self.self_attention(\n",
    "            x, x, x,\n",
    "            attn_mask=x_mask\n",
    "        )[0])\n",
    "        x, trend1 = self.decomp1(x)\n",
    "        x = x + self.dropout(self.cross_attention(\n",
    "            x, cross, cross,\n",
    "            attn_mask=cross_mask\n",
    "        )[0])\n",
    "        x, trend2 = self.decomp2(x)\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        x, trend3 = self.decomp3(x + y)\n",
    "\n",
    "        residual_trend = trend1 + trend2 + trend3\n",
    "        residual_trend = self.projection(residual_trend.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x, residual_trend\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, norm_layer=None, projection=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = norm_layer\n",
    "        self.projection = projection\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None, trend=None):\n",
    "        for layer in self.layers:\n",
    "            x, residual_trend = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n",
    "            trend = trend + residual_trend\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        if self.projection is not None:\n",
    "            x = self.projection(x)\n",
    "        return x, trend    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer is the first method to achieve the series-wise connection,\n",
    "    with inherent O(LlogL) complexity\n",
    "    \"\"\"\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.label_len = configs.label_len\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.output_attention = configs.output_attention\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.linear1 = nn.Linear(in_features= configs.c_in, out_features= configs.d_model, bias = True, device= 'cuda' if torch.cuda.is_available() else 'cpu' )\n",
    "        self.linear2 = nn.Linear(in_features= configs.c_in, out_features= configs.d_model, bias = True,  device= 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.linear3 = nn.Linear(in_features= configs.c_in, out_features= configs.c_out, bias = True,  device= 'cuda' if torch.cuda.is_available() else 'cpu')    \n",
    "        # Decomp\n",
    "        kernel_size = configs.moving_avg\n",
    "        self.decomp = series_decomp(kernel_size)\n",
    "\n",
    "        # Embedding\n",
    "        # The series-wise connection inherently contains the sequential information.\n",
    "        # Thus, we can discard the position embedding of transformers.\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AutoCorrelationLayer(\n",
    "                        AutoCorrelation(False, configs.factor, attention_dropout=configs.dropout,\n",
    "                                        output_attention=configs.output_attention),\n",
    "                        configs.d_model, configs.n_heads),\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    moving_avg=configs.moving_avg,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation\n",
    "                ) for l in range(configs.e_layers)\n",
    "            ],\n",
    "            norm_layer=my_Layernorm(configs.d_model)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AutoCorrelationLayer(\n",
    "                        AutoCorrelation(True, configs.factor, attention_dropout=configs.dropout,\n",
    "                                        output_attention=False),\n",
    "                        configs.d_model, configs.n_heads),\n",
    "                    AutoCorrelationLayer(\n",
    "                        AutoCorrelation(False, configs.factor, attention_dropout=configs.dropout,\n",
    "                                        output_attention=False),\n",
    "                        configs.d_model, configs.n_heads),\n",
    "                    configs.d_model,\n",
    "                    configs.c_out,\n",
    "                    configs.d_ff,\n",
    "                    moving_avg=configs.moving_avg,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation,\n",
    "                )\n",
    "                for l in range(configs.d_layers)\n",
    "            ],\n",
    "            norm_layer=my_Layernorm(configs.d_model),\n",
    "            projection=nn.Linear(configs.d_model, configs.c_out, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
    "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
    "        # decomp init\n",
    "        mean = torch.mean(x_enc, dim=1).unsqueeze(1).repeat(1, self.pred_len, 1)\n",
    "        zeros = torch.zeros([x_enc.shape[0], self.pred_len, x_enc.shape[2]], device=x_enc.device)\n",
    "        seasonal_init, trend_init = self.decomp(x_enc)\n",
    "        # decoder input\n",
    "        trend_init = torch.cat([trend_init[:, -self.label_len:, :], mean], dim=1)\n",
    "        seasonal_init = torch.cat([seasonal_init[:, -self.label_len:, :], zeros], dim=1)\n",
    "        # enc\n",
    "        x_enc = self.linear1(x_enc)\n",
    "        enc_out, attns = self.encoder(x_enc, attn_mask=enc_self_mask)\n",
    "        # dec\n",
    "        seasonal_init = self.linear2(seasonal_init)\n",
    "        trend_init = self.linear3(trend_init)\n",
    "        seasonal_part, trend_part = self.decoder(seasonal_init, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask,\n",
    "                                                 trend=trend_init)\n",
    "        # final\n",
    "        dec_out = trend_part + seasonal_part\n",
    "\n",
    "        if self.output_attention:\n",
    "            return dec_out[:, -self.pred_len:, :], attns\n",
    "        else:\n",
    "            return dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
    "        \n",
    "def predict(model, batch_x, batch_y, batch_x_mark, batch_y_mark):\n",
    "        dec_inp = torch.zeros_like(batch_y[:, -model.pred_len:, :]).float()\n",
    "        dec_inp = torch.cat([batch_y[:, :model.label_len, :], dec_inp], dim=1).float().to(model.device)     \n",
    "        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "        outputs = outputs[:, -model.pred_len:, :]\n",
    "        batch_y = batch_y[:, -model.pred_len:, :].to(model.device)\n",
    "        return outputs, batch_y    \n",
    "def valid(model,vali_loader, loss_fn):\n",
    "    model.eval()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    total_loss = []\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float()\n",
    "\n",
    "            batch_x_mark = batch_x_mark.float().to(device)\n",
    "            batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "            outputs, batch_y = predict(model,batch_x, batch_y, batch_x_mark, batch_y_mark)\n",
    "\n",
    "            pred = outputs.detach().cpu()\n",
    "            true = batch_y.detach().cpu()\n",
    "            loss = loss_fn(pred, true)\n",
    "            total_loss.append(loss)\n",
    "    total_loss = np.average(total_loss)\n",
    "    model.train()\n",
    "    return total_loss                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer,datapath, loss_fn, configs, epoch = 20,batch_size = 64):\n",
    "    data_train = MyData(size = [configs.seq_len, configs.label_len, configs.pred_len],path = datapath,flag = 'train')\n",
    "    data_test = MyData(size = [configs.seq_len, configs.label_len, configs.pred_len],path = datapath,flag = 'test' )\n",
    "    data_valid = MyData(size = [configs.seq_len, configs.label_len, configs.pred_len],path = datapath,flag = 'valid')\n",
    "    data_train = DataLoader(data_train, batch_size= batch_size, shuffle = True)\n",
    "    data_test = DataLoader(data_test, batch_size= batch_size, shuffle = False)\n",
    "    data_valid = DataLoader(data_valid, batch_size= batch_size, shuffle = False)\n",
    "    model.train()   \n",
    "    scheduler1 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9) \n",
    "    tmp = 0\n",
    "    for i in range(epoch):\n",
    "        train_loss = [] \n",
    "        for j, (seq_x, seq_y, seq_z, seq_t) in enumerate(data_train):\n",
    "            optimizer.zero_grad()\n",
    "            outputs, seq_y = predict(model, seq_x, seq_y, seq_z, seq_t)\n",
    "            loss = loss_fn(outputs, seq_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            if (j+1)%100 == 0:\n",
    "                print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(j + 1, i + 1, loss.item()))\n",
    "        scheduler1.step()    \n",
    "        train_loss = np.average(train_loss)\n",
    "        Valos = valid(model, data_valid, loss_fn)\n",
    "        Telos = valid(model, data_test, loss_fn)     \n",
    "        print(\"Epoch: {0} | Train Loss: {1:.7f} Vali Loss: {2:.7f} Test Loss: {3:.7f}\".format(\n",
    "                i + 1, train_loss, Valos, Telos))    \n",
    "        if i > 0:\n",
    "           if tmp > Valos:\n",
    "               print(\"Stop because valid_loss increase.\")\n",
    "               #break \n",
    "           else:\n",
    "               tmp = Valos \n",
    "               \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')  \n",
    "\n",
    "parser.add_argument('--seq_len', type = int, default = 100,help = 'input sequence length')\n",
    "parser.add_argument('--label_len', type = int,default = 50, help  = 'start token length')\n",
    "parser.add_argument('--pred_len', type = int, default = 100, help = 'prediction sequence length')\n",
    "parser.add_argument('--c_out', type = int, default = 4, help = 'output size')\n",
    "parser.add_argument('--c_in', type = int, default = 5, help = 'input size')\n",
    "parser.add_argument('--moving_avg', type = int,default = 15, help = 'window size')\n",
    "parser.add_argument('--output_attention', type = bool, default = False)\n",
    "parser.add_argument('--n_heads', type = int, default = 8)\n",
    "parser.add_argument('--d_model', type = int, default = 20)\n",
    "parser.add_argument('--e_layers', type = int, default = 2 )\n",
    "parser.add_argument('--d_layers', type = int, default = 1)\n",
    "parser.add_argument('--d_ff', type = int, default = 80)\n",
    "parser.add_argument('--factor', type = int, default = 1)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')#dropout\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "config = parser.parse_args(args = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = Model(config)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "train(model, optimizer, 'data/FPT.csv', nn.MSELoss(), config)\n",
    "torch.save(model.state_dict(), 'Auto_FPT.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = Model(config)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "train(model, optimizer, 'data/MSN.csv', nn.MSELoss(), config)\n",
    "torch.save(model.state_dict(), 'Auto_MSN.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = Model(config)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "train(model, optimizer, 'data/PNJ.csv', nn.MSELoss(), config)\n",
    "torch.save(model.state_dict(), 'Auto_PNJ.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = Model(config)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "train(model, optimizer, 'data/VIC.csv', nn.MSELoss(), config)\n",
    "torch.save(model.state_dict(), 'Auto_VIC.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mô hình RNN và LSTM\n",
    "Sử dụng chuỗi có độ dài seq_len = 100, dự báo 100 quan sát tiếp theo (pred_len = 100).  \n",
    "Các mô hình RNN huấn luyện với 20 epochs, các mô hình LSTM sẽ huấn luyện ít epoch hơn do trình trạng overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers,seq_len = 100, pred_len= 100):\n",
    "        super().__init__()\n",
    "        self.pred_len = pred_len\n",
    "        self.seq_len = seq_len \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, device = 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.fc = nn.Linear(hidden_dim,input_dim)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out)[:, -1:, :]  # chỉ lấy output ở time step cuối cùng\n",
    "        tmp = out\n",
    "        for i in range(self.pred_len -1):\n",
    "            out1, (hn, cn) = self.lstm(tmp, (hn, cn))\n",
    "            out1 = self.fc(out1)\n",
    "            out = torch.cat([out, out1], dim = 1)\n",
    "            tmp = out1\n",
    "        return out \n",
    "    \n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers,seq_len = 100, pred_len= 100):\n",
    "        super().__init__()\n",
    "        self.pred_len = pred_len\n",
    "        self.seq_len = seq_len \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers,batch_first=True,dropout = 0.05, device = 'cuda' if torch.cuda.is_available() else 'cpu', nonlinearity='relu')\n",
    "        self.fc = nn.Linear(hidden_dim,input_dim)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        out = self.fc(out)[:, -1:, :]  # chỉ lấy output ở time step cuối cùng\n",
    "        tmp = out\n",
    "        for i in range(self.pred_len -1):\n",
    "            out1, hn = self.rnn(tmp, hn)\n",
    "            out1 = self.fc(out1)\n",
    "            out = torch.cat([out, out1], dim = 1)\n",
    "            tmp = out1\n",
    "        return out     \n",
    "\n",
    "def predict_RNN(model, batch_x, batch_y):\n",
    "    true  = batch_y[:, -model.pred_len:, :].to(model.device)\n",
    "    pred = model(batch_x).to(model.device)\n",
    "    return pred, true\n",
    "\n",
    "def train_RNN(model, optimizer, datapath,loss_fn, epoch = 20, batch_size = 64):\n",
    "    data_train = MyData(size = [model.seq_len, 1, model.pred_len],path = datapath,flag = 'train', col_input= [ 2,3,4,5])\n",
    "    data_test = MyData(size = [model.seq_len, 1, model.pred_len],path = datapath,flag = 'test' , col_input = [2,3,4,5])\n",
    "    data_valid = MyData(size = [model.seq_len, 1, model.pred_len],path = datapath,flag = 'valid', col_input= [2,3,4,5])\n",
    "    data_train = DataLoader(data_train, batch_size= batch_size, shuffle = True)\n",
    "    data_test = DataLoader(data_test, batch_size= batch_size, shuffle = False)\n",
    "    data_valid = DataLoader(data_valid, batch_size= batch_size, shuffle = False)\n",
    "    model.train()   \n",
    "    scheduler1 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9) \n",
    "    tmp = 0\n",
    "    for i in range(epoch):\n",
    "        train_loss = [] \n",
    "        for j, (seq_x, seq_y, seq_z, seq_t) in enumerate(data_train):\n",
    "            optimizer.zero_grad()\n",
    "            outputs, seq_y = predict_RNN(model, seq_x, seq_y)\n",
    "            loss = loss_fn(outputs, seq_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            if (j+1)%100 == 0:\n",
    "                print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(j + 1, i + 1, loss.item()))\n",
    "        scheduler1.step()    \n",
    "        train_loss = np.average(train_loss)\n",
    "        Valos = valid_RNN(model, data_valid, loss_fn)\n",
    "        Telos = valid_RNN(model, data_test, loss_fn)     \n",
    "        print(\"Epoch: {0} | Train Loss: {1:.7f} Vali Loss: {2:.7f} Test Loss: {3:.7f}\".format(\n",
    "                i + 1, train_loss, Valos, Telos))    \n",
    "           \n",
    "def valid_RNN(model,vali_loader, loss_fn):\n",
    "    model.eval()\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    total_loss = []\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n",
    "            batch_x = batch_x.float().to(device)\n",
    "            batch_y = batch_y.float().to(device)\n",
    "            outputs, batch_y = predict_RNN(model,batch_x, batch_y)\n",
    "            pred = outputs.detach().cpu()\n",
    "            true = batch_y.detach().cpu()\n",
    "            loss = loss_fn(pred, true)\n",
    "            total_loss.append(loss)\n",
    "    total_loss = np.average(total_loss)\n",
    "    model.train()\n",
    "    return total_loss            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\titers: 100, epoch: 1 | loss: 189.5738525\n",
      "\titers: 200, epoch: 1 | loss: 148.6914215\n",
      "\titers: 300, epoch: 1 | loss: 114.1894836\n",
      "\titers: 400, epoch: 1 | loss: 85.0522690\n",
      "\titers: 500, epoch: 1 | loss: 55.7941856\n",
      "\titers: 600, epoch: 1 | loss: 41.9845314\n",
      "\titers: 700, epoch: 1 | loss: 32.7799644\n",
      "\titers: 800, epoch: 1 | loss: 23.3324585\n",
      "\titers: 900, epoch: 1 | loss: 21.1342144\n",
      "Epoch: 1 | Train Loss: 251.9109884 Vali Loss: 112.9227753 Test Loss: 193.9368591\n",
      "\titers: 100, epoch: 2 | loss: 22.2541752\n",
      "\titers: 200, epoch: 2 | loss: 25.4572678\n",
      "\titers: 300, epoch: 2 | loss: 20.1598129\n",
      "\titers: 400, epoch: 2 | loss: 26.1097641\n",
      "\titers: 500, epoch: 2 | loss: 21.9305916\n",
      "\titers: 600, epoch: 2 | loss: 33.7614708\n",
      "\titers: 700, epoch: 2 | loss: 22.3430099\n",
      "\titers: 800, epoch: 2 | loss: 28.9479485\n",
      "\titers: 900, epoch: 2 | loss: 18.0476971\n",
      "Epoch: 2 | Train Loss: 23.2549163 Vali Loss: 217.7062988 Test Loss: 323.9121399\n",
      "\titers: 100, epoch: 3 | loss: 15.8128443\n",
      "\titers: 200, epoch: 3 | loss: 17.5600529\n",
      "\titers: 300, epoch: 3 | loss: 16.9429131\n",
      "\titers: 400, epoch: 3 | loss: 13.5316210\n",
      "\titers: 500, epoch: 3 | loss: 17.2886810\n",
      "\titers: 600, epoch: 3 | loss: 28.4659805\n",
      "\titers: 700, epoch: 3 | loss: 14.8450670\n",
      "\titers: 800, epoch: 3 | loss: 15.9678011\n",
      "\titers: 900, epoch: 3 | loss: 16.3727703\n",
      "Epoch: 3 | Train Loss: 17.2357085 Vali Loss: 176.0823212 Test Loss: 250.9458008\n",
      "\titers: 100, epoch: 4 | loss: 18.8818207\n",
      "\titers: 200, epoch: 4 | loss: 9.1407194\n",
      "\titers: 300, epoch: 4 | loss: 11.0291195\n",
      "\titers: 400, epoch: 4 | loss: 9.9988804\n",
      "\titers: 500, epoch: 4 | loss: 10.3119535\n",
      "\titers: 600, epoch: 4 | loss: 9.3590469\n",
      "\titers: 700, epoch: 4 | loss: 8.3023129\n",
      "\titers: 800, epoch: 4 | loss: 10.9887362\n",
      "\titers: 900, epoch: 4 | loss: 13.3352299\n",
      "Epoch: 4 | Train Loss: 13.7325725 Vali Loss: 134.4642181 Test Loss: 185.8354034\n",
      "\titers: 100, epoch: 5 | loss: 9.9092026\n",
      "\titers: 200, epoch: 5 | loss: 10.4931602\n",
      "\titers: 300, epoch: 5 | loss: 8.9472904\n",
      "\titers: 400, epoch: 5 | loss: 8.2250204\n",
      "\titers: 500, epoch: 5 | loss: 8.0786304\n",
      "\titers: 600, epoch: 5 | loss: 15.8826199\n",
      "\titers: 700, epoch: 5 | loss: 10.6150475\n",
      "\titers: 800, epoch: 5 | loss: 48.3531609\n",
      "\titers: 900, epoch: 5 | loss: 16.9503727\n",
      "Epoch: 5 | Train Loss: 28.7124935 Vali Loss: 115.9366455 Test Loss: 189.8626556\n",
      "\titers: 100, epoch: 6 | loss: 17.5020943\n",
      "\titers: 200, epoch: 6 | loss: 18.6196003\n",
      "\titers: 300, epoch: 6 | loss: 15.1758118\n",
      "\titers: 400, epoch: 6 | loss: 12.3720388\n",
      "\titers: 500, epoch: 6 | loss: 11.3325939\n",
      "\titers: 600, epoch: 6 | loss: 11.3910522\n",
      "\titers: 700, epoch: 6 | loss: 12.6715746\n",
      "\titers: 800, epoch: 6 | loss: 11.7191286\n",
      "\titers: 900, epoch: 6 | loss: 6.5018263\n",
      "Epoch: 6 | Train Loss: 13.2344082 Vali Loss: 167.5732727 Test Loss: 231.5819397\n",
      "\titers: 100, epoch: 7 | loss: 10.6551266\n",
      "\titers: 200, epoch: 7 | loss: 7.2822328\n",
      "\titers: 300, epoch: 7 | loss: 8.8241873\n",
      "\titers: 400, epoch: 7 | loss: 9.4693880\n",
      "\titers: 500, epoch: 7 | loss: 15.2763758\n",
      "\titers: 600, epoch: 7 | loss: 6.0223460\n",
      "\titers: 700, epoch: 7 | loss: 14.8251686\n",
      "\titers: 800, epoch: 7 | loss: 6.3229008\n",
      "\titers: 900, epoch: 7 | loss: 7.3053856\n",
      "Epoch: 7 | Train Loss: 8.5238642 Vali Loss: 54.6306419 Test Loss: 73.7135315\n",
      "\titers: 100, epoch: 8 | loss: 6.8810825\n",
      "\titers: 200, epoch: 8 | loss: 6.5201240\n",
      "\titers: 300, epoch: 8 | loss: 5.6819696\n",
      "\titers: 400, epoch: 8 | loss: 6.5638356\n",
      "\titers: 500, epoch: 8 | loss: 5.3446898\n",
      "\titers: 600, epoch: 8 | loss: 5.8919978\n",
      "\titers: 700, epoch: 8 | loss: 8.7785120\n",
      "\titers: 800, epoch: 8 | loss: 4.9004803\n",
      "\titers: 900, epoch: 8 | loss: 92.8328247\n",
      "Epoch: 8 | Train Loss: 9.1334298 Vali Loss: 178.4531708 Test Loss: 241.5728455\n",
      "\titers: 100, epoch: 9 | loss: 5.1482310\n",
      "\titers: 200, epoch: 9 | loss: 8.3082962\n",
      "\titers: 300, epoch: 9 | loss: 6.0222688\n",
      "\titers: 400, epoch: 9 | loss: 6.2642336\n",
      "\titers: 500, epoch: 9 | loss: 5.4476757\n",
      "\titers: 600, epoch: 9 | loss: 4.9892340\n",
      "\titers: 700, epoch: 9 | loss: 4.6316829\n",
      "\titers: 800, epoch: 9 | loss: 7.4675484\n",
      "\titers: 900, epoch: 9 | loss: 7.0825291\n",
      "Epoch: 9 | Train Loss: 6.1949230 Vali Loss: 38.6860619 Test Loss: 48.3657722\n",
      "\titers: 100, epoch: 10 | loss: 6.3107214\n",
      "\titers: 200, epoch: 10 | loss: 9.0807772\n",
      "\titers: 300, epoch: 10 | loss: 3.9657218\n",
      "\titers: 400, epoch: 10 | loss: 7.4593272\n",
      "\titers: 500, epoch: 10 | loss: 6.2339110\n",
      "\titers: 600, epoch: 10 | loss: 4.0353303\n",
      "\titers: 700, epoch: 10 | loss: 5.3212295\n",
      "\titers: 800, epoch: 10 | loss: 4.3811936\n",
      "\titers: 900, epoch: 10 | loss: 5.3964472\n",
      "Epoch: 10 | Train Loss: 5.4423610 Vali Loss: 5.1065574 Test Loss: 6.1040468\n",
      "\titers: 100, epoch: 11 | loss: 4.7087345\n",
      "\titers: 200, epoch: 11 | loss: 3.5555758\n",
      "\titers: 300, epoch: 11 | loss: 3.5511920\n",
      "\titers: 400, epoch: 11 | loss: 3.5628703\n",
      "\titers: 500, epoch: 11 | loss: 3.5854514\n",
      "\titers: 600, epoch: 11 | loss: 4.1936183\n",
      "\titers: 700, epoch: 11 | loss: 4.3539729\n",
      "\titers: 800, epoch: 11 | loss: 5.1347594\n",
      "\titers: 900, epoch: 11 | loss: 6.2902112\n",
      "Epoch: 11 | Train Loss: 4.8783003 Vali Loss: 19.8168831 Test Loss: 23.6536369\n",
      "\titers: 100, epoch: 12 | loss: 3.2055836\n",
      "\titers: 200, epoch: 12 | loss: 3.4403446\n",
      "\titers: 300, epoch: 12 | loss: 3.1779292\n",
      "\titers: 400, epoch: 12 | loss: 3.5039294\n",
      "\titers: 500, epoch: 12 | loss: 2.9947343\n",
      "\titers: 600, epoch: 12 | loss: 3.3037157\n",
      "\titers: 700, epoch: 12 | loss: 3.1883402\n",
      "\titers: 800, epoch: 12 | loss: 4.2052660\n",
      "\titers: 900, epoch: 12 | loss: 3.1794119\n",
      "Epoch: 12 | Train Loss: 3.8634403 Vali Loss: 25.6749210 Test Loss: 30.5251045\n",
      "\titers: 100, epoch: 13 | loss: 7.1448779\n",
      "\titers: 200, epoch: 13 | loss: 2.9554780\n",
      "\titers: 300, epoch: 13 | loss: 2.8695478\n",
      "\titers: 400, epoch: 13 | loss: 2.7018435\n",
      "\titers: 500, epoch: 13 | loss: 3.1023982\n",
      "\titers: 600, epoch: 13 | loss: 2.8125336\n",
      "\titers: 700, epoch: 13 | loss: 3.0063410\n",
      "\titers: 800, epoch: 13 | loss: 3.1366889\n",
      "\titers: 900, epoch: 13 | loss: 3.0035944\n",
      "Epoch: 13 | Train Loss: 3.5223537 Vali Loss: 4.0144091 Test Loss: 4.4970269\n",
      "\titers: 100, epoch: 14 | loss: 2.2557023\n",
      "\titers: 200, epoch: 14 | loss: 2.7785904\n",
      "\titers: 300, epoch: 14 | loss: 2.9417646\n",
      "\titers: 400, epoch: 14 | loss: 3.0195746\n",
      "\titers: 500, epoch: 14 | loss: 2.2819526\n",
      "\titers: 600, epoch: 14 | loss: 3.3706033\n",
      "\titers: 700, epoch: 14 | loss: 2.9704843\n",
      "\titers: 800, epoch: 14 | loss: 2.2879140\n",
      "\titers: 900, epoch: 14 | loss: 4.8859129\n",
      "Epoch: 14 | Train Loss: 2.8769854 Vali Loss: 6.6513104 Test Loss: 7.2200499\n",
      "\titers: 100, epoch: 15 | loss: 2.2725463\n",
      "\titers: 200, epoch: 15 | loss: 3.2955358\n",
      "\titers: 300, epoch: 15 | loss: 6.8951354\n",
      "\titers: 400, epoch: 15 | loss: 2.3972363\n",
      "\titers: 500, epoch: 15 | loss: 2.0252621\n",
      "\titers: 600, epoch: 15 | loss: 5.7434692\n",
      "\titers: 700, epoch: 15 | loss: 2.1124661\n",
      "\titers: 800, epoch: 15 | loss: 1.9055573\n",
      "\titers: 900, epoch: 15 | loss: 1.7648265\n",
      "Epoch: 15 | Train Loss: 2.6700912 Vali Loss: 1.5678880 Test Loss: 2.4308183\n",
      "\titers: 100, epoch: 16 | loss: 2.0567067\n",
      "\titers: 200, epoch: 16 | loss: 1.7066237\n",
      "\titers: 300, epoch: 16 | loss: 3.1425464\n",
      "\titers: 400, epoch: 16 | loss: 3.1339507\n",
      "\titers: 500, epoch: 16 | loss: 2.0447948\n",
      "\titers: 600, epoch: 16 | loss: 2.2597780\n",
      "\titers: 700, epoch: 16 | loss: 2.3923459\n",
      "\titers: 800, epoch: 16 | loss: 1.9634541\n",
      "\titers: 900, epoch: 16 | loss: 4.2028532\n",
      "Epoch: 16 | Train Loss: 2.2470237 Vali Loss: 3.3003843 Test Loss: 3.3619325\n",
      "\titers: 100, epoch: 17 | loss: 1.7574519\n",
      "\titers: 200, epoch: 17 | loss: 1.5261123\n",
      "\titers: 300, epoch: 17 | loss: 1.6895474\n",
      "\titers: 400, epoch: 17 | loss: 1.4109507\n",
      "\titers: 500, epoch: 17 | loss: 2.1579523\n",
      "\titers: 600, epoch: 17 | loss: 1.4380652\n",
      "\titers: 700, epoch: 17 | loss: 3.5706701\n",
      "\titers: 800, epoch: 17 | loss: 1.7415094\n",
      "\titers: 900, epoch: 17 | loss: 1.6121231\n",
      "Epoch: 17 | Train Loss: 1.9131449 Vali Loss: 3.3254666 Test Loss: 3.4890499\n",
      "\titers: 100, epoch: 18 | loss: 1.7076955\n",
      "\titers: 200, epoch: 18 | loss: 1.5962657\n",
      "\titers: 300, epoch: 18 | loss: 2.3629019\n",
      "\titers: 400, epoch: 18 | loss: 1.5776050\n",
      "\titers: 500, epoch: 18 | loss: 2.2727518\n",
      "\titers: 600, epoch: 18 | loss: 1.3238035\n",
      "\titers: 700, epoch: 18 | loss: 3.0444186\n",
      "\titers: 800, epoch: 18 | loss: 1.9071754\n",
      "\titers: 900, epoch: 18 | loss: 1.5475372\n",
      "Epoch: 18 | Train Loss: 1.7602329 Vali Loss: 3.5280440 Test Loss: 3.6909864\n",
      "\titers: 100, epoch: 19 | loss: 1.7237344\n",
      "\titers: 200, epoch: 19 | loss: 1.2684404\n",
      "\titers: 300, epoch: 19 | loss: 1.2031170\n",
      "\titers: 400, epoch: 19 | loss: 1.3570569\n",
      "\titers: 500, epoch: 19 | loss: 1.5774224\n",
      "\titers: 600, epoch: 19 | loss: 1.4043843\n",
      "\titers: 700, epoch: 19 | loss: 1.2551227\n",
      "\titers: 800, epoch: 19 | loss: 2.9034965\n",
      "\titers: 900, epoch: 19 | loss: 1.6683258\n",
      "Epoch: 19 | Train Loss: 1.5374435 Vali Loss: 3.2053895 Test Loss: 4.9642167\n",
      "\titers: 100, epoch: 20 | loss: 1.3445507\n",
      "\titers: 200, epoch: 20 | loss: 1.2010041\n",
      "\titers: 300, epoch: 20 | loss: 1.3773111\n",
      "\titers: 400, epoch: 20 | loss: 1.1074114\n",
      "\titers: 500, epoch: 20 | loss: 1.0900512\n",
      "\titers: 600, epoch: 20 | loss: 1.0149510\n",
      "\titers: 700, epoch: 20 | loss: 1.5927533\n",
      "\titers: 800, epoch: 20 | loss: 1.7457820\n",
      "\titers: 900, epoch: 20 | loss: 1.1176333\n",
      "Epoch: 20 | Train Loss: 1.4076269 Vali Loss: 3.8242869 Test Loss: 5.8062315\n"
     ]
    }
   ],
   "source": [
    "model = MyRNN(input_dim = 4, hidden_dim = 20, num_layers = 3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "train_RNN(model, optimizer, 'data/FPT.csv', nn.MSELoss())\n",
    "torch.save(model.state_dict(), 'RNN_FPT.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyRNN(input_dim = 4, hidden_dim = 20, num_layers = 3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "train_RNN(model, optimizer, 'data/PNJ.csv', nn.MSELoss())\n",
    "torch.save(model.state_dich(), 'RNN_PNJ.path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\titers: 100, epoch: 1 | loss: 6622.4335938\n",
      "\titers: 200, epoch: 1 | loss: 583.0628052\n",
      "\titers: 300, epoch: 1 | loss: 178.7241821\n",
      "\titers: 400, epoch: 1 | loss: 163.8155823\n",
      "\titers: 500, epoch: 1 | loss: 114.2703247\n",
      "\titers: 600, epoch: 1 | loss: 104.1965790\n",
      "\titers: 700, epoch: 1 | loss: 98.9813461\n",
      "\titers: 800, epoch: 1 | loss: 92.8153992\n",
      "\titers: 900, epoch: 1 | loss: 76.4600830\n",
      "\titers: 1000, epoch: 1 | loss: 82.0717316\n",
      "\titers: 1100, epoch: 1 | loss: 117.1894836\n",
      "\titers: 1200, epoch: 1 | loss: 73.7060852\n",
      "Epoch: 1 | Train Loss: 844.5808205 Vali Loss: 36.1170273 Test Loss: 120.7970123\n",
      "\titers: 100, epoch: 2 | loss: 74.9657898\n",
      "\titers: 200, epoch: 2 | loss: 78.8224106\n",
      "\titers: 300, epoch: 2 | loss: 56.8160706\n",
      "\titers: 400, epoch: 2 | loss: 63.8375206\n",
      "\titers: 500, epoch: 2 | loss: 57.6279106\n",
      "\titers: 600, epoch: 2 | loss: 71.9108276\n",
      "\titers: 700, epoch: 2 | loss: 63.4866905\n",
      "\titers: 800, epoch: 2 | loss: 68.6710281\n",
      "\titers: 900, epoch: 2 | loss: 64.1615372\n",
      "\titers: 1000, epoch: 2 | loss: 45.2296181\n",
      "\titers: 1100, epoch: 2 | loss: 40.7235451\n",
      "\titers: 1200, epoch: 2 | loss: 39.7254524\n",
      "Epoch: 2 | Train Loss: 60.0188594 Vali Loss: 95.4088211 Test Loss: 247.0949402\n",
      "\titers: 100, epoch: 3 | loss: 59.6186180\n",
      "\titers: 200, epoch: 3 | loss: 40.7120552\n",
      "\titers: 300, epoch: 3 | loss: 40.8119011\n",
      "\titers: 400, epoch: 3 | loss: 38.4808693\n",
      "\titers: 500, epoch: 3 | loss: 43.3027496\n",
      "\titers: 600, epoch: 3 | loss: 37.3794479\n",
      "\titers: 700, epoch: 3 | loss: 51.0377731\n",
      "\titers: 800, epoch: 3 | loss: 48.3678703\n",
      "\titers: 900, epoch: 3 | loss: 44.1304893\n",
      "\titers: 1000, epoch: 3 | loss: 42.9921761\n",
      "\titers: 1100, epoch: 3 | loss: 30.3614922\n",
      "\titers: 1200, epoch: 3 | loss: 62.3264503\n",
      "Epoch: 3 | Train Loss: 41.1563331 Vali Loss: 113.6955338 Test Loss: 273.9712219\n",
      "\titers: 100, epoch: 4 | loss: 36.4638290\n",
      "\titers: 200, epoch: 4 | loss: 43.9164543\n",
      "\titers: 300, epoch: 4 | loss: 32.5496788\n",
      "\titers: 400, epoch: 4 | loss: 26.7260475\n",
      "\titers: 500, epoch: 4 | loss: 28.4332561\n",
      "\titers: 600, epoch: 4 | loss: 21.2517548\n",
      "\titers: 700, epoch: 4 | loss: 30.0385914\n",
      "\titers: 800, epoch: 4 | loss: 25.5043945\n",
      "\titers: 900, epoch: 4 | loss: 24.2007008\n",
      "\titers: 1000, epoch: 4 | loss: 26.6652298\n",
      "\titers: 1100, epoch: 4 | loss: 153.9183655\n",
      "\titers: 1200, epoch: 4 | loss: 26.4249687\n",
      "Epoch: 4 | Train Loss: 30.2429354 Vali Loss: 133.5957642 Test Loss: 294.9340820\n",
      "\titers: 100, epoch: 5 | loss: 28.2154484\n",
      "\titers: 200, epoch: 5 | loss: 24.2941284\n",
      "\titers: 300, epoch: 5 | loss: 32.5346489\n",
      "\titers: 400, epoch: 5 | loss: 28.0179539\n",
      "\titers: 500, epoch: 5 | loss: 17.9650841\n",
      "\titers: 600, epoch: 5 | loss: 57.2752914\n",
      "\titers: 700, epoch: 5 | loss: 20.7564011\n",
      "\titers: 800, epoch: 5 | loss: 19.1547031\n",
      "\titers: 900, epoch: 5 | loss: 14.8362913\n",
      "\titers: 1000, epoch: 5 | loss: 29.3528271\n",
      "\titers: 1100, epoch: 5 | loss: 15.7855749\n",
      "\titers: 1200, epoch: 5 | loss: 12.3816395\n",
      "Epoch: 5 | Train Loss: 26.7551505 Vali Loss: 148.8689575 Test Loss: 304.3561401\n",
      "\titers: 100, epoch: 6 | loss: 17.3623104\n",
      "\titers: 200, epoch: 6 | loss: 13.5693588\n",
      "\titers: 300, epoch: 6 | loss: 12.8630056\n",
      "\titers: 400, epoch: 6 | loss: 20.0332737\n",
      "\titers: 500, epoch: 6 | loss: 21.9559307\n",
      "\titers: 600, epoch: 6 | loss: 17.3171520\n",
      "\titers: 700, epoch: 6 | loss: 15.1780319\n",
      "\titers: 800, epoch: 6 | loss: 40.7753296\n",
      "\titers: 900, epoch: 6 | loss: 13.4358053\n",
      "\titers: 1000, epoch: 6 | loss: 13.7749977\n",
      "\titers: 1100, epoch: 6 | loss: 13.7095995\n",
      "\titers: 1200, epoch: 6 | loss: 17.8531075\n",
      "Epoch: 6 | Train Loss: 20.1455503 Vali Loss: 167.2119598 Test Loss: 326.5997620\n",
      "\titers: 100, epoch: 7 | loss: 12.4377670\n",
      "\titers: 200, epoch: 7 | loss: 14.4552670\n",
      "\titers: 300, epoch: 7 | loss: 11.3293295\n",
      "\titers: 400, epoch: 7 | loss: 10.6107121\n",
      "\titers: 500, epoch: 7 | loss: 23.1368179\n",
      "\titers: 600, epoch: 7 | loss: 13.0909910\n",
      "\titers: 700, epoch: 7 | loss: 24.9545517\n",
      "\titers: 800, epoch: 7 | loss: 46.6079445\n",
      "\titers: 900, epoch: 7 | loss: 12.5668287\n",
      "\titers: 1000, epoch: 7 | loss: 29.0706863\n",
      "\titers: 1100, epoch: 7 | loss: 69.1456146\n",
      "\titers: 1200, epoch: 7 | loss: 11.2005777\n",
      "Epoch: 7 | Train Loss: 18.4450954 Vali Loss: 151.8901215 Test Loss: 292.0860596\n",
      "\titers: 100, epoch: 8 | loss: 30.5826035\n",
      "\titers: 200, epoch: 8 | loss: 9.3572788\n",
      "\titers: 300, epoch: 8 | loss: 11.0330725\n",
      "\titers: 400, epoch: 8 | loss: 13.5509644\n",
      "\titers: 500, epoch: 8 | loss: 10.1994514\n",
      "\titers: 600, epoch: 8 | loss: 8.9255333\n",
      "\titers: 700, epoch: 8 | loss: 19.8241844\n",
      "\titers: 800, epoch: 8 | loss: 11.4871635\n",
      "\titers: 900, epoch: 8 | loss: 14.1302099\n",
      "\titers: 1000, epoch: 8 | loss: 11.5507889\n",
      "\titers: 1100, epoch: 8 | loss: 14.4242411\n",
      "\titers: 1200, epoch: 8 | loss: 11.1127968\n",
      "Epoch: 8 | Train Loss: 14.5387652 Vali Loss: 295.7576599 Test Loss: 495.9677429\n",
      "\titers: 100, epoch: 9 | loss: 7.8652973\n",
      "\titers: 200, epoch: 9 | loss: 11.4557238\n",
      "\titers: 300, epoch: 9 | loss: 12.3317957\n",
      "\titers: 400, epoch: 9 | loss: 9.6156893\n",
      "\titers: 500, epoch: 9 | loss: 9.7993698\n",
      "\titers: 600, epoch: 9 | loss: 10.2030811\n",
      "\titers: 700, epoch: 9 | loss: 9.9976101\n",
      "\titers: 800, epoch: 9 | loss: 8.6149693\n",
      "\titers: 900, epoch: 9 | loss: 8.9043055\n",
      "\titers: 1000, epoch: 9 | loss: 10.8998499\n",
      "\titers: 1100, epoch: 9 | loss: 7.3111353\n",
      "\titers: 1200, epoch: 9 | loss: 7.7277703\n",
      "Epoch: 9 | Train Loss: 12.1946634 Vali Loss: 225.9717255 Test Loss: 385.1282349\n",
      "\titers: 100, epoch: 10 | loss: 7.3048363\n",
      "\titers: 200, epoch: 10 | loss: 25.9120865\n",
      "\titers: 300, epoch: 10 | loss: 8.4508228\n",
      "\titers: 400, epoch: 10 | loss: 8.4175978\n",
      "\titers: 500, epoch: 10 | loss: 5.8774414\n",
      "\titers: 600, epoch: 10 | loss: 9.1850443\n",
      "\titers: 700, epoch: 10 | loss: 5.6567669\n",
      "\titers: 800, epoch: 10 | loss: 5.6063275\n",
      "\titers: 900, epoch: 10 | loss: 6.2929859\n",
      "\titers: 1000, epoch: 10 | loss: 9.0595436\n",
      "\titers: 1100, epoch: 10 | loss: 11.5809879\n",
      "\titers: 1200, epoch: 10 | loss: 6.7384505\n",
      "Epoch: 10 | Train Loss: 10.0463843 Vali Loss: 233.7736206 Test Loss: 385.9214172\n",
      "\titers: 100, epoch: 11 | loss: 9.5023508\n",
      "\titers: 200, epoch: 11 | loss: 5.6058288\n",
      "\titers: 300, epoch: 11 | loss: 7.3624439\n",
      "\titers: 400, epoch: 11 | loss: 6.8483424\n",
      "\titers: 500, epoch: 11 | loss: 7.4663148\n",
      "\titers: 600, epoch: 11 | loss: 7.0590849\n",
      "\titers: 700, epoch: 11 | loss: 8.6686506\n",
      "\titers: 800, epoch: 11 | loss: 8.7841225\n",
      "\titers: 900, epoch: 11 | loss: 14.8717794\n",
      "\titers: 1000, epoch: 11 | loss: 7.0365472\n",
      "\titers: 1100, epoch: 11 | loss: 5.5058680\n",
      "\titers: 1200, epoch: 11 | loss: 6.2679515\n",
      "Epoch: 11 | Train Loss: 8.6656530 Vali Loss: 233.4683380 Test Loss: 376.6691284\n",
      "\titers: 100, epoch: 12 | loss: 8.1560421\n",
      "\titers: 200, epoch: 12 | loss: 6.4393044\n",
      "\titers: 300, epoch: 12 | loss: 6.9212670\n",
      "\titers: 400, epoch: 12 | loss: 7.8178339\n",
      "\titers: 500, epoch: 12 | loss: 6.4572735\n",
      "\titers: 600, epoch: 12 | loss: 6.7446346\n",
      "\titers: 700, epoch: 12 | loss: 4.7059131\n",
      "\titers: 800, epoch: 12 | loss: 4.6553912\n",
      "\titers: 900, epoch: 12 | loss: 10.2085752\n",
      "\titers: 1000, epoch: 12 | loss: 8.1516876\n",
      "\titers: 1100, epoch: 12 | loss: 4.8203850\n",
      "\titers: 1200, epoch: 12 | loss: 8.2468386\n",
      "Epoch: 12 | Train Loss: 8.5434905 Vali Loss: 217.9258728 Test Loss: 348.2791748\n",
      "\titers: 100, epoch: 13 | loss: 6.4458861\n",
      "\titers: 200, epoch: 13 | loss: 5.3641200\n",
      "\titers: 300, epoch: 13 | loss: 7.6907496\n",
      "\titers: 400, epoch: 13 | loss: 10.4543858\n",
      "\titers: 500, epoch: 13 | loss: 5.3551502\n",
      "\titers: 600, epoch: 13 | loss: 4.8777351\n",
      "\titers: 700, epoch: 13 | loss: 6.9995480\n",
      "\titers: 800, epoch: 13 | loss: 11.6128597\n",
      "\titers: 900, epoch: 13 | loss: 3.5229540\n",
      "\titers: 1000, epoch: 13 | loss: 9.8034220\n",
      "\titers: 1100, epoch: 13 | loss: 10.7760448\n",
      "\titers: 1200, epoch: 13 | loss: 7.4147716\n",
      "Epoch: 13 | Train Loss: 6.3266486 Vali Loss: 187.1256256 Test Loss: 295.7133179\n",
      "\titers: 100, epoch: 14 | loss: 5.2545457\n",
      "\titers: 200, epoch: 14 | loss: 6.7984829\n",
      "\titers: 300, epoch: 14 | loss: 3.9047775\n",
      "\titers: 400, epoch: 14 | loss: 7.4249654\n",
      "\titers: 500, epoch: 14 | loss: 9.1803493\n",
      "\titers: 600, epoch: 14 | loss: 6.8594141\n",
      "\titers: 700, epoch: 14 | loss: 3.4468567\n",
      "\titers: 800, epoch: 14 | loss: 3.4385052\n",
      "\titers: 900, epoch: 14 | loss: 7.0901418\n",
      "\titers: 1000, epoch: 14 | loss: 3.5961895\n",
      "\titers: 1100, epoch: 14 | loss: 5.8192272\n",
      "\titers: 1200, epoch: 14 | loss: 3.9455271\n",
      "Epoch: 14 | Train Loss: 6.0027569 Vali Loss: 216.9933777 Test Loss: 335.2322388\n",
      "\titers: 100, epoch: 15 | loss: 8.7986364\n",
      "\titers: 200, epoch: 15 | loss: 4.7114816\n",
      "\titers: 300, epoch: 15 | loss: 3.5833802\n",
      "\titers: 400, epoch: 15 | loss: 5.0930223\n",
      "\titers: 500, epoch: 15 | loss: 6.4599071\n",
      "\titers: 600, epoch: 15 | loss: 5.2337904\n",
      "\titers: 700, epoch: 15 | loss: 4.2880068\n",
      "\titers: 800, epoch: 15 | loss: 3.1351733\n",
      "\titers: 900, epoch: 15 | loss: 4.9550548\n",
      "\titers: 1000, epoch: 15 | loss: 4.4894733\n",
      "\titers: 1100, epoch: 15 | loss: 5.9064651\n",
      "\titers: 1200, epoch: 15 | loss: 6.7514172\n",
      "Epoch: 15 | Train Loss: 5.1397434 Vali Loss: 177.7048950 Test Loss: 273.6952820\n",
      "\titers: 100, epoch: 16 | loss: 4.2151442\n",
      "\titers: 200, epoch: 16 | loss: 5.7249217\n",
      "\titers: 300, epoch: 16 | loss: 2.9653363\n",
      "\titers: 400, epoch: 16 | loss: 3.2279823\n",
      "\titers: 500, epoch: 16 | loss: 6.3936806\n",
      "\titers: 600, epoch: 16 | loss: 5.5011683\n",
      "\titers: 700, epoch: 16 | loss: 3.0683656\n",
      "\titers: 800, epoch: 16 | loss: 6.7138782\n",
      "\titers: 900, epoch: 16 | loss: 2.8023119\n",
      "\titers: 1000, epoch: 16 | loss: 3.6971335\n",
      "\titers: 1100, epoch: 16 | loss: 2.7457650\n",
      "\titers: 1200, epoch: 16 | loss: 3.0020273\n",
      "Epoch: 16 | Train Loss: 4.7752121 Vali Loss: 165.4288330 Test Loss: 254.6441956\n",
      "\titers: 100, epoch: 17 | loss: 3.4857852\n",
      "\titers: 200, epoch: 17 | loss: 3.1970789\n",
      "\titers: 300, epoch: 17 | loss: 3.4403558\n",
      "\titers: 400, epoch: 17 | loss: 8.6545944\n",
      "\titers: 500, epoch: 17 | loss: 3.1513817\n",
      "\titers: 600, epoch: 17 | loss: 2.7909205\n",
      "\titers: 700, epoch: 17 | loss: 3.4713166\n",
      "\titers: 800, epoch: 17 | loss: 5.3028870\n",
      "\titers: 900, epoch: 17 | loss: 9.5118399\n",
      "\titers: 1000, epoch: 17 | loss: 3.3933430\n",
      "\titers: 1100, epoch: 17 | loss: 6.5228577\n",
      "\titers: 1200, epoch: 17 | loss: 3.2496166\n",
      "Epoch: 17 | Train Loss: 4.2908372 Vali Loss: 141.7066040 Test Loss: 218.8822632\n",
      "\titers: 100, epoch: 18 | loss: 3.9453399\n",
      "\titers: 200, epoch: 18 | loss: 3.0077970\n",
      "\titers: 300, epoch: 18 | loss: 3.6979721\n",
      "\titers: 400, epoch: 18 | loss: 3.7608843\n",
      "\titers: 500, epoch: 18 | loss: 7.8156037\n",
      "\titers: 600, epoch: 18 | loss: 3.8593454\n",
      "\titers: 700, epoch: 18 | loss: 3.4916964\n",
      "\titers: 800, epoch: 18 | loss: 4.6414394\n",
      "\titers: 900, epoch: 18 | loss: 2.5693870\n",
      "\titers: 1000, epoch: 18 | loss: 6.0343652\n",
      "\titers: 1100, epoch: 18 | loss: 3.5983822\n",
      "\titers: 1200, epoch: 18 | loss: 2.7035162\n",
      "Epoch: 18 | Train Loss: 3.9938752 Vali Loss: 152.8705444 Test Loss: 234.3993225\n",
      "\titers: 100, epoch: 19 | loss: 3.3774085\n",
      "\titers: 200, epoch: 19 | loss: 2.5287354\n",
      "\titers: 300, epoch: 19 | loss: 4.9305482\n",
      "\titers: 400, epoch: 19 | loss: 3.5479002\n",
      "\titers: 500, epoch: 19 | loss: 2.4733214\n",
      "\titers: 600, epoch: 19 | loss: 3.7580242\n",
      "\titers: 700, epoch: 19 | loss: 3.8968534\n",
      "\titers: 800, epoch: 19 | loss: 3.7990415\n",
      "\titers: 900, epoch: 19 | loss: 4.0424852\n",
      "\titers: 1000, epoch: 19 | loss: 3.5624893\n",
      "\titers: 1100, epoch: 19 | loss: 2.6216919\n",
      "\titers: 1200, epoch: 19 | loss: 3.3723364\n",
      "Epoch: 19 | Train Loss: 3.6496780 Vali Loss: 138.6315765 Test Loss: 211.9438324\n",
      "\titers: 100, epoch: 20 | loss: 2.7121389\n",
      "\titers: 200, epoch: 20 | loss: 2.8463507\n",
      "\titers: 300, epoch: 20 | loss: 4.1122327\n",
      "\titers: 400, epoch: 20 | loss: 3.2006540\n",
      "\titers: 500, epoch: 20 | loss: 3.7960458\n",
      "\titers: 600, epoch: 20 | loss: 2.4630845\n",
      "\titers: 700, epoch: 20 | loss: 2.5608335\n",
      "\titers: 800, epoch: 20 | loss: 3.6809278\n",
      "\titers: 900, epoch: 20 | loss: 3.8574667\n",
      "\titers: 1000, epoch: 20 | loss: 2.8880298\n",
      "\titers: 1100, epoch: 20 | loss: 5.8000398\n",
      "\titers: 1200, epoch: 20 | loss: 6.9606647\n",
      "Epoch: 20 | Train Loss: 3.5326042 Vali Loss: 143.2458649 Test Loss: 218.2242737\n"
     ]
    }
   ],
   "source": [
    "model = MyRNN(input_dim = 4, hidden_dim = 20, num_layers = 3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "train_RNN(model, optimizer, 'data/MSN.csv', nn.MSELoss())\n",
    "torch.save(model.state_dich(), 'RNN_MSN.path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyRNN(input_dim = 4, hidden_dim = 20, num_layers = 3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "train_RNN(model, optimizer, 'data/VIC.csv', nn.MSELoss())\n",
    "torch.save(model.state_dict(), 'RNN_VIC.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\titers: 100, epoch: 1 | loss: 5613.9194336\n",
      "\titers: 200, epoch: 1 | loss: 5047.3032227\n",
      "\titers: 300, epoch: 1 | loss: 4794.2011719\n",
      "\titers: 400, epoch: 1 | loss: 4419.8500977\n",
      "\titers: 500, epoch: 1 | loss: 4162.2441406\n",
      "\titers: 600, epoch: 1 | loss: 3754.7656250\n",
      "\titers: 700, epoch: 1 | loss: 3593.7434082\n",
      "\titers: 800, epoch: 1 | loss: 3605.7614746\n",
      "\titers: 900, epoch: 1 | loss: 3249.2385254\n",
      "\titers: 1000, epoch: 1 | loss: 2874.4970703\n",
      "\titers: 1100, epoch: 1 | loss: 2727.0830078\n",
      "Epoch: 1 | Train Loss: 4011.4997946 Vali Loss: 2099.0446777 Test Loss: 1618.6068115\n",
      "\titers: 100, epoch: 2 | loss: 2416.5615234\n",
      "\titers: 200, epoch: 2 | loss: 2464.5654297\n",
      "\titers: 300, epoch: 2 | loss: 2147.6345215\n",
      "\titers: 400, epoch: 2 | loss: 2230.4106445\n",
      "\titers: 500, epoch: 2 | loss: 1930.6160889\n",
      "\titers: 600, epoch: 2 | loss: 1909.6711426\n",
      "\titers: 700, epoch: 2 | loss: 1672.3828125\n",
      "\titers: 800, epoch: 2 | loss: 1574.6566162\n",
      "\titers: 900, epoch: 2 | loss: 1406.9144287\n",
      "\titers: 1000, epoch: 2 | loss: 1335.0128174\n",
      "\titers: 1100, epoch: 2 | loss: 1242.4862061\n",
      "Epoch: 2 | Train Loss: 1839.1467922 Vali Loss: 856.8821411 Test Loss: 538.4063110\n",
      "\titers: 100, epoch: 3 | loss: 1158.8981934\n",
      "\titers: 200, epoch: 3 | loss: 982.8591919\n",
      "\titers: 300, epoch: 3 | loss: 873.1135254\n",
      "\titers: 400, epoch: 3 | loss: 804.5002441\n",
      "\titers: 500, epoch: 3 | loss: 861.5822144\n",
      "\titers: 600, epoch: 3 | loss: 729.2426758\n",
      "\titers: 700, epoch: 3 | loss: 731.5675049\n",
      "\titers: 800, epoch: 3 | loss: 578.5968628\n",
      "\titers: 900, epoch: 3 | loss: 627.5484619\n",
      "\titers: 1000, epoch: 3 | loss: 513.2083740\n",
      "\titers: 1100, epoch: 3 | loss: 492.4689331\n",
      "Epoch: 3 | Train Loss: 766.1640960 Vali Loss: 327.3567505 Test Loss: 132.6100311\n",
      "\titers: 100, epoch: 4 | loss: 390.2843018\n",
      "\titers: 200, epoch: 4 | loss: 418.4964600\n",
      "\titers: 300, epoch: 4 | loss: 360.5284729\n",
      "\titers: 400, epoch: 4 | loss: 339.0862122\n",
      "\titers: 500, epoch: 4 | loss: 279.3548584\n",
      "\titers: 600, epoch: 4 | loss: 263.8506775\n",
      "\titers: 700, epoch: 4 | loss: 233.8126373\n",
      "\titers: 800, epoch: 4 | loss: 208.3350067\n",
      "\titers: 900, epoch: 4 | loss: 149.2941742\n",
      "\titers: 1000, epoch: 4 | loss: 214.7950592\n",
      "\titers: 1100, epoch: 4 | loss: 190.1178131\n",
      "Epoch: 4 | Train Loss: 277.8737707 Vali Loss: 170.2486420 Test Loss: 65.7064285\n"
     ]
    }
   ],
   "source": [
    "model = MyLSTM(input_dim = 4, hidden_dim = 20, num_layers = 3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "train_RNN(model, optimizer, 'data/PNJ.csv', nn.MSELoss(), epoch = 4)\n",
    "torch.save(model.state_dict(), 'LSTM_PNJ.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyLSTM(input_dim = 4, hidden_dim = 20, num_layers = 3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "train_RNN(model, optimizer, 'data/FPT.csv', nn.MSELoss(), epoch = 4)\n",
    "torch.save(model.state_dict(), 'LSTM_FPT.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyLSTM(input_dim = 4, hidden_dim = 20, num_layers = 3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "train_RNN(model, optimizer, 'data/MSN.csv', nn.MSELoss(), epoch = 4)\n",
    "torch.save(model.state_dict(), 'LSTM_MSN.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyLSTM(input_dim = 4, hidden_dim = 20, num_layers = 3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "train_RNN(model, optimizer, 'data/VIC.csv', nn.MSELoss(), epoch = 4)\n",
    "torch.save(model.state_dict(), 'LSTM_VIC.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model và tính toán trên tập data test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected state_dict to be dict-like, got <class 'method'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m data_test \u001b[38;5;241m=\u001b[39m MyData(size \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m], path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/FPT.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(config)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAuto_FPT.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:2104\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \n\u001b[0;32m   2071\u001b[0m \u001b[38;5;124;03mIf :attr:`strict` is ``True``, then\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2101\u001b[0m \u001b[38;5;124;03m    ``RuntimeError``.\u001b[39;00m\n\u001b[0;32m   2102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state_dict, Mapping):\n\u001b[1;32m-> 2104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected state_dict to be dict-like, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(state_dict)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2106\u001b[0m missing_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2107\u001b[0m unexpected_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected state_dict to be dict-like, got <class 'method'>."
     ]
    }
   ],
   "source": [
    "data_test = MyData(size = [100, 50, 100], path='data/FPT.csv', flag = 'test')\n",
    "model = Model(config)\n",
    "model.load_state_dict(torch.load('Auto_FPT.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load('Auto_FPT.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of Model(\n",
       "  (linear1): Linear(in_features=5, out_features=20, bias=True)\n",
       "  (linear2): Linear(in_features=5, out_features=20, bias=True)\n",
       "  (linear3): Linear(in_features=5, out_features=4, bias=True)\n",
       "  (decomp): series_decomp(\n",
       "    (moving_avg): moving_avg(\n",
       "      (avg): AvgPool1d(kernel_size=(15,), stride=(1,), padding=(0,))\n",
       "    )\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (attn_layers): ModuleList(\n",
       "      (0-1): 2 x EncoderLayer(\n",
       "        (attention): AutoCorrelationLayer(\n",
       "          (inner_correlation): AutoCorrelation(\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=20, out_features=16, bias=True)\n",
       "          (key_projection): Linear(in_features=20, out_features=16, bias=True)\n",
       "          (value_projection): Linear(in_features=20, out_features=16, bias=True)\n",
       "          (out_projection): Linear(in_features=16, out_features=20, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(20, 80, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (conv2): Conv1d(80, 20, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (decomp1): series_decomp(\n",
       "          (moving_avg): moving_avg(\n",
       "            (avg): AvgPool1d(kernel_size=(15,), stride=(1,), padding=(0,))\n",
       "          )\n",
       "        )\n",
       "        (decomp2): series_decomp(\n",
       "          (moving_avg): moving_avg(\n",
       "            (avg): AvgPool1d(kernel_size=(15,), stride=(1,), padding=(0,))\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): my_Layernorm(\n",
       "      (layernorm): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attention): AutoCorrelationLayer(\n",
       "          (inner_correlation): AutoCorrelation(\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=20, out_features=16, bias=True)\n",
       "          (key_projection): Linear(in_features=20, out_features=16, bias=True)\n",
       "          (value_projection): Linear(in_features=20, out_features=16, bias=True)\n",
       "          (out_projection): Linear(in_features=16, out_features=20, bias=True)\n",
       "        )\n",
       "        (cross_attention): AutoCorrelationLayer(\n",
       "          (inner_correlation): AutoCorrelation(\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (query_projection): Linear(in_features=20, out_features=16, bias=True)\n",
       "          (key_projection): Linear(in_features=20, out_features=16, bias=True)\n",
       "          (value_projection): Linear(in_features=20, out_features=16, bias=True)\n",
       "          (out_projection): Linear(in_features=16, out_features=20, bias=True)\n",
       "        )\n",
       "        (conv1): Conv1d(20, 80, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (conv2): Conv1d(80, 20, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (decomp1): series_decomp(\n",
       "          (moving_avg): moving_avg(\n",
       "            (avg): AvgPool1d(kernel_size=(15,), stride=(1,), padding=(0,))\n",
       "          )\n",
       "        )\n",
       "        (decomp2): series_decomp(\n",
       "          (moving_avg): moving_avg(\n",
       "            (avg): AvgPool1d(kernel_size=(15,), stride=(1,), padding=(0,))\n",
       "          )\n",
       "        )\n",
       "        (decomp3): series_decomp(\n",
       "          (moving_avg): moving_avg(\n",
       "            (avg): AvgPool1d(kernel_size=(15,), stride=(1,), padding=(0,))\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "        (projection): Conv1d(20, 4, kernel_size=(3,), stride=(1,), padding=(1,), bias=False, padding_mode=circular)\n",
       "      )\n",
       "    )\n",
       "    (norm): my_Layernorm(\n",
       "      (layernorm): LayerNorm((20,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (projection): Linear(in_features=20, out_features=4, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
